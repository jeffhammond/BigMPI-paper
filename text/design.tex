% !TEX root = ../bigmpi.tex

\section{Design}


MPI point-to-point routines, both two-sided and one-sided, were
trivial to implement via MPI datatypes.  This class of routines provides the
most commonly used MPI functionality, so for many codes the Forum has been
proven correct.  However, as we will see in Section~\ref{sec:reductions},
not all parts of the MPI standard will be so straightforward.

BigMPI implements all variants of send and receive, blocking and nonblocking variants of
the homogeneous collectives (bcast, gather, scatter, allgather, alltoall)
and RMA (put, get, accumulate, get\_accumulate)
along the lines of the example for MPI\_Send, shown in Figure~\ref{code:mpi_send_x}.

\begin{figure}
\begin{code}
int MPIX_Send_x(const void *buf, MPI_Count count,
                MPI_Datatype dt, int dest,
                int tag, MPI_Comm comm)
{
    int rc = MPI_SUCCESS;
    if (likely (count <= INT_MAX )) {
        rc = MPI_Send(buf, (int)count, dt, dest, tag, comm);
    } else {
        MPI_Datatype newtype;
        MPIX_Type_contiguous_x(count, dt, &newtype);
        MPI_Type_commit(&newtype);
        rc = MPI_Send(buf, 1, newtype, dest, tag, comm);
        MPI_Type_free(&newtype);
    }
    return rc;
}
\end{code}
\caption{The implementation of large-count Send, which serves as a template
for many other MPI-3 routines.\label{code:mpi_send_x}}
\end{figure}

The critical function in all of the large-count implementations noted above
is \texttt{MPIX\_Type\_contiguous\_x}, which emits a single datatype that
represents up to \texttt{SIZE\_MAX} elements.
%Supporting more elements than can fit into $2^{63}$ bytes of memory is
%not necessary, since such a system is unlikely to exist in the foreseeable future.
%However, since we now express these parameters in our own datatypes, we introduce
%a degree of flexibility not currently present in the MPI standard.
This utility routine allows us to implement large-count support in a straightforward
fashion since all instances of $(large\_count,type)$ are mapped to $(1,large\_type)$
by this function.
An associated decoder function extracts the original $large\_count$ from a
user-defined datatype; this function is employed within the user-defined reduction
operations.  Decoding a datatype is nontrivial even for such a simple case --
we must call \texttt{MPI\_Type\_get\_envelope} and \texttt{MPI\_Type\_get\_contents}
three times each just to unwind the result of \texttt{MPIX\_Type\_contiguous\_x}.
BigMPI is boon to the majority of application programmers that are unfamiliar 
with such features in the MPI standard by virtue of hiding these details.

Other datatypes can be supported easily within BigMPI, but this is not a high
priority because the primary goal is to solve the large-count problem for users
that are not currently making use of derived datatypes.
A user that employs derived datatypes in their code already is likely capable
of implementing their own large-count support already.
Nonetheless, the release version of BigMPI will support large-count equivalents
of all of the existing datatype constructors.

\begin{figure}
\begin{code}
int MPIX_Type_contiguous_x(MPI_Count count, 
                           MPI_Datatype oldtype, 
                           MPI_Datatype * newtype)
{
    assert(count<SIZE_MAX); /* has to fit into MPI_Aint */
    MPI_Count c = count/INT_MAX, r = count%INT_MAX;

    MPI_Datatype chunks, remainder;
    MPI_Type_vector(c, INT_MAX, INT_MAX, oldtype, &chunks);
    MPI_Type_contiguous(r, oldtype, &remainder);

    MPI_Aint lb /* unused */, extent;
    MPI_Type_get_extent(oldtype, &lb, &extent);

    MPI_Aint remdisp          = (MPI_Aint)c*INT_MAX*extent;
    int blklens[2]            = {1,1};
    MPI_Aint disps[2]         = {0,remdisp};
    MPI_Datatype types[2]     = {chunks,remainder};
    MPI_Type_create_struct(2, blklens, disps, types, newtype);

    MPI_Type_free(&chunks);
    MPI_Type_free(&remainder);

    return MPI_SUCCESS;
}
\end{code}
\label{code:type_contig_x}
\caption{Function for construction a large-count contiguous datatype.
A vector type describes a series of adjacent chunks, and a struct type picks up
any remaining data in case the count is not evenly divisible.}
\end{figure}

%In the general case, MPI\_Type\_create\_struct is required, although BigMPI tries to factorize the count into C integers so we can use MPI\_Type\_vector.

%Ticket 423 would improve user experience because it addresses the common case of large counts of built-ins.

\input{text/reductions}
\input{text/v-collectives}
\input{text/n-collectives}
\input{text/interface}
\input{text/limitations}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}

\texttt{MPIX\_Type\_contiguous\_x}
does the heavy lifting.  It's pretty obvious how it works.
A quality datatype engine will turn this into a contiguous datatype internally 
and thus the underlying communication will be efficient.  
This approach assumes a count-safe MPI implementation, but implementations need
to be count-safe period if the Forum is serious about datatypes being
the solution rather than \texttt{MPI\_Count} everywhere.

All of the communication functions follow the same pattern, demonstrated in
Figure~\ref{code:mpi_send_x}.
\end{comment}


\subsection{Details}

BigMPI is optimized for the common case when count is smaller than $2^{31}$
with a \texttt{likely\_if} macro to minimize the performance hit for
this more common use case.  Our aim is for users to call the BigMPI routines
directly, instead of inserting a branch for the large-count case themselves.

To optimize further, one could implement
\texttt{MPIX\_Type\_contiguous\_x} using internals of the MPI implementation
instead of calling six MPI datatype functions.  Considering we are operating on
gigabytes of data, the performance gains one might wrest from such an approach
would be limited.


