% !TEX root = ../bigmpi.tex

\section{Suggestions for MPI-4}
\label{sec:mpi4}

\subsubsection{Reductions}

Whether or not one can apply a built-in reduce operation to a simple 
(e.g.\ contiguous and homogeneous) user-defined datatype
is a fundamental inconsistency in the MPI standard, as accumulate
functions permit this while reductions do not.
% (1) 34+338 (+339?) required for sane reductions.
Tickets 34~\cite{ticket34} and 338~\cite{ticket338}
% https://svn.mpi-forum.org/trac/mpi-forum-web/ticket/338
propose to reconcile reductions and accumulate by generalizing
reductions to include the features of accumulate (but not the converse).
Both BigMPI and the popular numerical library PETSc wish to leverage 
``accumulate-style behavior'' in reductions, i.e.\  the built-in operations 
can work on user-defined datatypes in an element-wise basis.

Ticket 339~\cite{ticket339} % https://svn.mpi-forum.org/trac/mpi-forum-web/ticket/339
(``User-defined op with derived datatypes yields space-inefficient reduce'')
is related to the problem with \texttt{MPI\_IN\_PLACE} with user-defined reductions.
A more general interface for user-defined reduction operations that supports both
in-place and pipelined reductions would be of great value to BigMPI.

While creating a large-count contiguous datatype seems like a simple thing to do,
the n{\"a}ive implementation encounters overflow issues without explicit casting and is
thus error-prone.  In any case, the implementation of this feature on top of MPI
requires six MPI functions, whereas the internal implementation would be almost trivial,
as it would merely set the internal count on the datatype -- a field that will not overflow 
if the implementation is count-safe.
Adopting ticket 423~\cite{ticket423} % https://svn.mpi-forum.org/trac/mpi-forum-web/ticket/423
(``add MPI\_Type\_contiguous\_x'') will reduce user difficulty when dealing with large counts.
As is evidenced by BigMPI and the prototyped implementation within MPICH, 
the change is straightforward to implement.

%430 is required to support nonblocking v-collectives in a straightforward way.
When applying BigMPI's large-count strategy to the v-collectives, the (counts[], type) description 
has to be mapped to (newcount[], newtypes[]), and that in turn requires the w-variants.
Ticket 430~\cite{ticket430}
% https://svn.mpi-forum.org/trac/mpi-forum-web/ticket/430
(``large-count v-collectives'') 
would provide a large-count v-collective and would avoid the need for big temporary memory allocations.
It also solves the problem associated with \texttt{int} displacements in \texttt{MPI\_Alltoallv}, which lead to
an overflow issue even if each process sends less than $2^{31}$ elements.
For example, a parallel FFT on 12GB of C \texttt{float} will overflow because the value of the displacements
for approximately one-third of the processes exceed $2^{31}$.

Finally, the implementation of non-blocking collectives using point-to-point --
which is the most straightforward solution in many cases -- 
requires improved generalized requests.
Ticket 457~\cite{ticket457} % https://svn.mpi-forum.org/trac/mpi-forum-web/ticket/457
(``expose progress in generalized requests'') is an older proposal to address well-known
issues with generalized requests that has long been solved in MPICH but is not standardized.

Note that we do not propose to add large-count versions of all MPI communication routines,
as was suggested but ultimately rejected during MPI-3 discussions.
Many of the most popular MPI functions work just fine with the datatypes solution and
the addition of \texttt{MPI\_Type\_contiguous\_x} would make it almost trivial for users to
realize large-count support in applications.
Where we have proposed a set of new functions -- large-count v-collectives -- it is because
the overhead of emulating this support on top of MPI-3 is $O(n_{proc})$ and the semantic mismatch is
profound (e.g.\ large-count \texttt{MPI\_Scatterv} as \texttt{MPI\_Neighborhood\_alltoallw} is unnatural).

