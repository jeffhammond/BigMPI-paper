% !TEX root = ../bigmpi.tex

\section{Background}

The MPI Forum resolved the large-count issue by deciding the standard already
had sufficient routines to support large-count workloads, and left the matter
of presentation to a vaguely specified additional library.  We took up this
challenge, writing a prototype implementation of
essentially all of the communication functionality in MPI-3.  Our interface
differs from standard MPI only in promoting the ``count'' of the (count,
datatype) pair to the larger MPI\_Count type.  The software implementation is
found in the open-source BigMPI
library and is relatively friendly to users, i.e.\ it has both a Cmake
and an Autotools build system and requires a generic programming
environment composed of a C99 compiler and an implementation
of MPI-3.

MPI's point-to-point routines, both two-sided and one-sided, were
trivial to implement via MPI datatypes.  This class of routines provides the
most commonly used MPI functionality, so for many codes the Forum has been
proven correct.  As we will see in Section~\ref{sec:reductions},
not all parts of the MPI
standard will be so straightforward.

BigMPI implements all variants of send and receive, blocking and nonblocking variants of
the homogeneous collectives (bcast, gather, scatter, allgather, alltoall)
and RMA (put, get, accumulate, get\_accumulate)
along the lines of the example for MPI\_Send, shown in Figure~\ref{code:mpi_send_x}.

\begin{figure}
\begin{code}
int MPIX_Send_x(const void *buf, MPI_Count count,
                MPI_Datatype dt, int dest,
                int tag, MPI_Comm comm)
{
    int rc = MPI_SUCCESS;
    if (likely (count <= INT_MAX )) {
        rc = MPI_Send(buf, (int)count, dt, dest, tag, comm);
    } else {
        MPI_Datatype newtype;
        MPIX_Type_contiguous_x(count, dt, &newtype);
        MPI_Type_commit(&newtype);
        rc = MPI_Send(buf, 1, newtype, dest, tag, comm);
        MPI_Type_free(&newtype);
    }
    return rc;
}
\end{code}
\label{code:mpi_send_x}
\caption{The implementation of large-count Send, which serves as a template
for many other MPI-3 routines.}
\end{figure}

The critical function in all of the large-count implementations noted above
is \texttt{MPIX\_Type\_contiguous\_x}, which emits a single datatype that
represents up to \texttt{SIZE\_MAX} elements.
Supporting more elements than can fit into $2^{63}$ bytes of memory is
not necessary, since such a system is unlikely to exist in the foreseeable future.
However, since we now express these parameters in our own datatypes, we introduce
a degree of flexibility not currently present in the MPI standard.



\begin{figure}
\begin{code}
int MPIX_Type_contiguous_x(MPI_Count count, 
                           MPI_Datatype oldtype, 
                           MPI_Datatype * newtype)
{
    assert(count<SIZE_MAX); /* has to fit into MPI_Aint */
    MPI_Count c = count/INT_MAX, r = count%INT_MAX;

    MPI_Datatype chunks, remainder;
    MPI_Type_vector(c, INT_MAX, INT_MAX, oldtype, &chunks);
    MPI_Type_contiguous(r, oldtype, &remainder);

    MPI_Aint lb /* unused */, extent;
    MPI_Type_get_extent(oldtype, &lb, &extent);

    MPI_Aint remdisp          = (MPI_Aint)c*INT_MAX*extent;
    int blklens[2]            = {1,1};
    MPI_Aint disps[2]         = {0,remdisp};
    MPI_Datatype types[2]     = {chunks,remainder};
    MPI_Type_create_struct(2,
    	blklens, disps, types, newtype);

    MPI_Type_free(&chunks);
    MPI_Type_free(&remainder);

    return MPI_SUCCESS;
}
\end{code}
\label{code:type_contig_x}
\caption{Function for construction a large-count contiguous datatype.
A vector type describes a series of adjacent chunks, and a struct type picks up
any remaining data in case the count is not evenly divisible.}
\end{figure}

%In the general case, MPI\_Type\_create\_struct is required, although BigMPI tries to factorize the count into C integers so we can use MPI\_Type\_vector.

%Ticket 423 would improve user experience because it addresses the common case of large counts of built-ins.

\subsection{Reductions}
\label{sec:reductions}

Large-count support for reductions poses a challenge, particularly in the nonblocking case.
The MPI standard stipulates that built-in reduction operations can be used with built-in types
in the case of reductions, which means that performing a reduction on a vector of $N$
doubles using count=$N$ and type=\texttt{MPI\_DOUBLE} is compatible with \texttt{MPI\_SUM},
whereas the same reduction performed using a contiguous datatype to represent the vector
of doubles requires a user-defined reduction operation.
Thus, BigMPI creates user-defined operations corresponding to all the built-in reductions
acting on contiguous datatypes.  Inside of these reduction operations, the datatype is
decoded and the reduction performed using multiple calls to \texttt{MPI\_Reduce\_local}
and the appropriate built-in reduction operation.

Since many MPI implementations have optimized implementations of reductions for
built-in reduction operations, BigMPI turns a single call for a large count into multiple
calls with count less than $2^{31}$, but this only works for the blocking case.
It is not possible to return a single request object associated with more than one
nonblocking operation.  
Generalized requests - the MPI-standard way to implement non-blocking operations 
in a library - are not a viable alternative for reasons that have been documented 
in other work (\cite{latham:grequest-extensions}).

Another negative consequence of user-defined reductions is that  \texttt{MPI\_IN\_PLACE}
cannot be supported.  The user-defined reduce function interface (see below) does not expose
the information required to do an arbitrary in-place reduction.
\begin{code}
MPI_User_function(void* invec, void* inoutvec, 
                  int *len, MPI_Datatype *datatype);
\end{code}
The breaking up a single large-count reduction into multiple calls avoids this
issue with user-defined reductions, but it is only viable for the blocking case.

\subsection{Vector-argument collectives}

Vector-argument collectives (henceforth v-collectives) are the generalization of 
\texttt{MPI\_Scatter}, \texttt{MPI\_Gather}, \texttt{MPI\_Alltoall} etc.
when the count but not the datatype varies across processes.
When datatypes are used to support large counts, all of these operations must be
mapped to \texttt{MPI\_Alltoallw} because each large count will be mapped
to a different user-defined datatype.
Using \texttt{MPI\_Alltoallw} to implement, e.g., a large-count \texttt{MPI\_Scatterv} is
particularly inefficient because the former assumes inputs from every process,
whereas the latter only uses the input from the root.
However, the overhead of scanning a vector of counts where all but one is zero
is inconsequential compared to the cost of transmitting a buffer of $2^{31}$ bytes.
%Additionally, such vectors are unlikely to be particularly long since scattering
%a buffer that would run into large-count issues r

%All V-collectives turn into ALLTOALLW because different counts implies different large-count types.

The v-collectives encounter a second, more subtle issue due to the mapping to
\texttt{MPI\_Alltoallw}.  Because this function takes a vector of datatypes, the
displacements into the input and output vectors are given in bytes,
not element count, and the type of this offset is a C integer.
This creates an overflow situation \textit{even in the case
where the input buffer is less than 2 GiB} because a vector of one billion
alternating integers and floats may require an byte-offset in excess of $2^{31}$.
Thus, \texttt{MPI\_Alltoallw} is not an acceptable solution for the large-count
v-collectives because of the likelihood of overflowing in the displacement vector.
The use of C integer instead of \texttt{MPI\_Aint} for the displacement vector in
the  collective operations added prior to MPI-3 is a unfortunate historical artifact,
as \texttt{MPI\_Aint} was not introduced until \todo{When was AINT introduced?}.

%ALLTOALLW displacements given in bytes are C int and therefore it is impossible to offset more than 2GB into the buffer.
%NEIGHBOR\_ALLTOALLW to the rescue?!?!
%MPI\_Neighbor\_alltoallw displacements are MPI\_Aint not int.  This is good.
%Neighborhood collectives require special communicators that must be created for each call (and possibly cached).
%Must allocate new argument vectors and, in the case of alltoall, we wastefully splat the same value in all locations.

Fortunately, the overflow issue with displacements in \texttt{MPI\_Alltoallw} is
resolved using the neighborhood collectives introduced in MPI-3, which
use \texttt{MPI\_Aint} for displacements.
On the other hand, neighborhood collectives require an appropriate
communicator, which must be constructed prior to calling \texttt{MPI\_Neighborhood\_alltoallw}.
BigMPI creates a distributed graph communicator using \texttt{MPI\_Dist\_graph\_adjacent}
on-the-fly for every invocation of the large-count v-collectives, which is assumed to be
insignificant overhead compared to the data movement entailed in such an operation.

Thus, the implementation of large-count v-collectives using \texttt{MPI\_Neighborhood\_alltoallw} 
requires two $O(n_{proc})$ setup steps: the first is to create the vectors of send and receive
counts, displacements and datatypes and the second is to create a distributed graph communicator.
Creating the vector of datatypes requires $O(n_{proc})$ calls to \texttt{BigMPI\_Type\_contiguous\_x},
which itself requires six MPI calls, although all of these are expected to be inexpensive.

% V-collectives using P2P and RMA
%One can follow the definition in MPI to implement all of the V-collectives using P2P.
%RMA (with win\_fence synchronization) also works for the V-collectives.
%Allgatherv using nproc calls to Bcast also works.
%Large-count definitely outside of recursive doubling regime so little to optimize...

An alternative approach to implementing large-count v-collectives is to map
these to point-to-point operations, although this only works for blocking operations
for the reasons noted above.
Given that large-count v-collectives are well outside of the regime where latency-oriented 
optimizations like recursive-doubling are important, this is unlikely to have a significant impact 
on performance and it eliminates the need for the $O(n_{proc})$ setup steps.
The MPI standard describes every collective in terms of its implementation 
in terms of send-recv calls; the BigMPI implementation in terms of point-to-point
follows these recipes closely.
Nonblocking receives are pre-posted by the root or all ranks as appropriate;
the root or all ranks then call nonblocking send; all ranks then call Waitall.
As the large-count BigMPI send-recv functions are used, there is no need for
$O(n_{proc})$ vectors of datatypes, etc.; only a vector of \texttt{MPI\_Request}
objects for the nonblocking operations is required.



% V-collectives - nonblocking issues

None of the aforementioned solutions works for nonblocking because:

What request do we return in the case of P2P or RMA?

Cannot free argument vectors until complete.

Any solution involving generalized requests is untenable for users.  BigMPI might use it.

\subsection{Neighborhood collectives}

Scalar collectives are easy.

V-collectives: map to ALLTOALLW

Same problem as before with nonblocking regarding the allocated argument vectors.

If not for MPI\_Aint displacements in ALLTOALLW, we would have to drop into P2P and MPICH generalized requests.
