% !TEX root = ../bigmpi.tex

\section{Background}

The MPI standard provides a wide range of communication functions that
take a C \texttt{int} argument for the element count, thereby limiting this
value to \texttt{INT\_MAX} or less.
Hence, one cannot send, for example. 3 billion bytes using the \texttt{MPI\_BYTE} 
datatype or a vector of 5 billion integers using the \texttt{MPI\_INT} type. 

These limitations may seem academic: 2 billion
\texttt{MPI\_DOUBLE} equate to 16 GB, and one might think that
applications may rarely need to transmit that much data, since there
may be less memory available for the whole address space in which the MPI
process is running.
Two recent trends may render this limit increasingly impractical, however: 
first, growing core counts per CPU mean larger data portions per MPI 
process, and second, Big Data applications may demand more memory 
per core than the traditional 2 GB for computer simulations.

If the user code manually packs data, either for
performance~\cite{jenkins2012enabling} or for encoding
reasons~(\cite{boostmpi} \cite{li:PnetCDF},
 then the MPI implementation may be given just
an array of \texttt{MPI\_BYTE}s, which further reduces the maximum
message size (e.g., 250 million for C \texttt{double}).

A natural workaround is to use MPI derived datatypes. While 
application developers are likely to know typical data sizes and
can thus intercept calls that may exceed the \texttt{INT\_MAX} limit,
another scenario is harder to solve: problem solving
environments~\cite{cactus:SC01, gromacs} and computational
libraries~\cite{physis, libgeodecomp} operate on data structures with
user-defined dimensions. To ensure correctness, developers would need
to safeguard all communication functions that operate on user data.

%As we will demonstrate in the following sections, this is not a
%trivial task for all communication operations. A generic, reusable
%solution as provided by BigMPI alleviates the required implementation
%effort; library users can use preprocessor directives in a header file
%to select the appropriate set of communication functions:
%
%\begin{code}
%// configuration header:
%#ifdef BIGMPI
%#define MPIX_Bcast_x MPIX_Bcast_x
%#define MPIX_Send_x MPIX_Send_x
%...
%#else // cannot use count>INT_MAX
%#define MPIX_Bcast_x MPI_Bcast
%#define MPIX_Send_x MPI_Send
%...
%#endif
%\end{code}

%This project aspires to make it as easy as possible to support arbitrarily
%large counts ($2^{63}$ elements exceeds the local storage capacity of computers
%for the foreseeable future).

% This text sucks

% One area where MPI is less extensive than desirable is operations on very large data.
% Specifically, MPI uses the native integer in the C (\texttt{int}) and Fortran (\texttt{INTEGER})
% interfaces to represent the number of data elements to be communicated;
% in some cases, this type is also used to express an offset (more on this later).  

This paper focuses on the issues with the C interface, and we use the
well-known convention I$n_{I}$L$n_{L}$P$n_{P}$ to refer to the sizes
of the C types \texttt{int}, \texttt{long}, and \texttt{void*}, respectively.
For ILP32 systems, the largest buffer one can allocate is $2^{32}$ bytes (4 GiB),
while MPI can handle buffers of up to 2 GiB; the factor of 2 difference is
almost never a problem since 4 GiB of \texttt{int}, for example, requires
a count of only $2^{30}$.
 A problem emerges in IL32P64 and I32LP64 systems because one can allocate
 more memory in a buffer than can be captured with an integer count and built-in datatype.
 For example, a vector of 3 billion floats requires 12 GB of memory but cannot be 
 communicated with any communication routine using built-in datatypes.

% previously in background
%The MPI Forum resolved the large-count issue by deciding the standard already
%had sufficient routines to support large-count workloads, and left the matter
%of presentation to a vaguely specified additional library.  
%We took up this challenge, writing a prototype implementation of
%essentially all of the communication functionality in MPI-3.  
