% !TEX root = ../bigmpi.tex

\section{Background}

THIS PROBABLY DOES NOT BELONG HERE

In order to understand the implementation issues associated with
large-count support in MPI, we prototyped an implementation of
essentially all of the communication functionality in MPI-3 so as to 
support large counts.  
The software implementation is found in the open-source BigMPI
library and is relatively friendly to users, i.e. it has both a Cmake
and an Autotools build system and requires a generic programming
environment composed of a C99 compiler and an implementation
of MPI-3.

The most common functionality in MPI -- including all of the point-to-point,
which includes both two-sided and one-sided in this context -- were
trivial w.r.t. large-count support via MPI datatypes.

All variants of send and receive, blocking and nonblocking variants of
the homogeneous collectives (bcast, gather, scatter, allgather, alltoall)
and RMA (put, get, accumulate, get\_accumulate) were implemented
using the simple shown in Figure~\ref{code:mpi_send_x}.

\begin{figure}
\begin{code}
int MPIX_Send_x(const void *buf, MPI_Count count,
                MPI_Datatype dt, int dest,
                int tag, MPI_Comm comm)
{
    int rc = MPI_SUCCESS;
    if (likely (count <= INT_MAX )) {
        rc = MPI_Send(buf, (int)count, dt, dest, tag, comm);
    } else {
        MPI_Datatype newtype;
        MPIX_Type_contiguous_x(count, dt, &newtype);
        MPI_Type_commit(&newtype);
        rc = MPI_Send(buf, 1, newtype, dest, tag, comm);
        MPI_Type_free(&newtype);
    }
    return rc;
}
\end{code}
\label{code:mpi_send_x}
\caption{The implementation of large-count Send, which serves as a template
for many other routines in MPI-3.}
\end{figure}

\begin{figure}
\begin{code}
int MPIX_Type_contiguous_x(MPI_Count count, 
                           MPI_Datatype oldtype, 
                           MPI_Datatype * newtype)
{
    assert(count<SIZE_MAX); /* has to fit into MPI_Aint */
    MPI_Count c = count/INT_MAX, r = count%INT_MAX;

    MPI_Datatype chunks, remainder;
    MPI_Type_vector(c, INT_MAX, INT_MAX, oldtype, &chunks);
    MPI_Type_contiguous(r, oldtype, &remainder);

    MPI_Aint lb /* unused */, extent;
    MPI_Type_get_extent(oldtype, &lb, &extent);

    MPI_Aint remdisp          = (MPI_Aint)c*INT_MAX*extent;
    int blklens[2]            = {1,1};
    MPI_Aint disps[2]         = {0,remdisp};
    MPI_Datatype types[2]     = {chunks,remainder};
    MPI_Type_create_struct(2, blklens, disps, types, newtype);

    MPI_Type_free(&chunks);
    MPI_Type_free(&remainder);

    return MPI_SUCCESS;
}
\end{code}
\label{code:type_contig_x}
\caption{Function for construction a large-count contiguous datatype.}
\end{figure}

In the general case, MPI\_Type\_create\_struct is required, although BigMPI tries to factorize the count into C integers so we can use MPI\_Type\_vector.

Ticket 423 would improve user experience because it addresses the common case of large counts of built-ins.

\subsection{Reductions}

Reductions were not easy

User-defined ops required for user-defined types.  This would be fixed by tickets 34+338.

How do I support MPI\_IN\_PLACE inside of my user-defined reduction?
I want to use MPI\_Reduce\_local?

Blocking reductions can use the cleaver \todo{what the heck is a cleaver?}

\subsection{Vector-argument collectives}

All V-collectives turn into ALLTOALLW because different counts implies different large-count types.

ALLTOALLW displacements given in bytes are C int and therefore it is impossible to offset more than 2GB into the buffer.

NEIGHBOR\_ALLTOALLW to the rescue?!?!

MPI\_Neighbor\_alltoallw displacements are MPI\_Aint not int.  This is good.

Neighborhood collectives require special communicators that must be created for each call (and possibly cached).

Must allocate new argument vectors and, in the case of ?!alltoall?, we wastefully splat the same value in all locations.

% V-collectives using P2P and RMA

One can follow the definition in MPI to implement all of the V-collectives using P2P.

RMA (with win\_fence synchronization) also works for the V-collectives.

Allgatherv using nproc calls to Bcast also works.

Large-count definitely outside of recursive doubling regime so little to optimize...

% V-collectives - nonblocking issues

None of the aforementioned solutions works for nonblocking because:

What request do we return in the case of P2P or RMA?

Cannot free argument vectors until complete.

Any solution involving generalized requests is untenable for users.  BigMPI might use it.

\subsection{Neighborhood collectives}

Scalar collectives are easy.

V-collectives: map to ALLTOALLW

Same problem as before with nonblocking regarding the allocated argument vectors.

If not for MPI\_Aint displacements in ALLTOALLW, we would have to drop into P2P and MPICH generalized requests.
