% !TEX root = ../bigmpi.tex

\section{Background}

THIS PROBABLY DOES NOT BELONG HERE

The MPI Forum resolved the large-count issue by deciding the standard already
had sufficient routines to support large-count workloads, and left the matter
of presentation to a vaguely specified additional library.  We took up this
challenge, writing a prototype implementation of
essentially all of the communication functionality in MPI-3.  Our interface
differs from standard MPI only in promoting the ``count'' of the (count,
datatype) pair to the larger MPI\_Count type.  The software implementation is
found in the open-source BigMPI
library and is relatively friendly to users, i.e.\ it has both a Cmake
and an Autotools build system and requires a generic programming
environment composed of a C99 compiler and an implementation
of MPI-3.

MPI's point-to-point routines, both two-sided and one-sided, were
trivial to implement via MPI datatypes.  This class of routines provides the
most commonly used MPI functionality, so for many codes the Forum has been
proven correct.  As we will see in Section~\ref{sec:reductions},
not all parts of the MPI
standard will be so straightforward.

BigMPI implements all variants of send and receive, blocking and nonblocking variants of
the homogeneous collectives (bcast, gather, scatter, allgather, alltoall)
and RMA (put, get, accumulate, get\_accumulate)
along the lines of the example for MPI\_Send, shown in Figure~\ref{code:mpi_send_x}.

\begin{figure}
\begin{code}
int MPIX_Send_x(const void *buf, MPI_Count count,
                MPI_Datatype dt, int dest,
                int tag, MPI_Comm comm)
{
    int rc = MPI_SUCCESS;
    if (likely (count <= INT_MAX )) {
        rc = MPI_Send(buf, (int)count, dt, dest, tag, comm);
    } else {
        MPI_Datatype newtype;
        MPIX_Type_contiguous_x(count, dt, &newtype);
        MPI_Type_commit(&newtype);
        rc = MPI_Send(buf, 1, newtype, dest, tag, comm);
        MPI_Type_free(&newtype);
    }
    return rc;
}
\end{code}
\label{code:mpi_send_x}
\caption{The implementation of large-count Send, which serves as a template
for many other MPI-3 routines.}
\end{figure}

The critical function in all of the large-count implementations noted above
is \texttt{MPIX\_Type\_contiguous\_x}, which emits a single datatype that
represents up to \texttt{SIZE\_MAX} elements.
Supporting more elements than can fit into $2^{63}$ bytes of memory is
not necessary, since such a system is unlikely to exist in the foreseeable future.
However, since we now express these parameters in our own datatypes, we introduce
a degree of flexibility not currently present in the MPI standard.



\begin{figure}
\begin{code}
int MPIX_Type_contiguous_x(MPI_Count count, 
                           MPI_Datatype oldtype, 
                           MPI_Datatype * newtype)
{
    assert(count<SIZE_MAX); /* has to fit into MPI_Aint */
    MPI_Count c = count/INT_MAX, r = count%INT_MAX;

    MPI_Datatype chunks, remainder;
    MPI_Type_vector(c, INT_MAX, INT_MAX, oldtype, &chunks);
    MPI_Type_contiguous(r, oldtype, &remainder);

    MPI_Aint lb /* unused */, extent;
    MPI_Type_get_extent(oldtype, &lb, &extent);

    MPI_Aint remdisp          = (MPI_Aint)c*INT_MAX*extent;
    int blklens[2]            = {1,1};
    MPI_Aint disps[2]         = {0,remdisp};
    MPI_Datatype types[2]     = {chunks,remainder};
    MPI_Type_create_struct(2, blklens, disps, types, newtype);

    MPI_Type_free(&chunks);
    MPI_Type_free(&remainder);

    return MPI_SUCCESS;
}
\end{code}
\label{code:type_contig_x}
\caption{Function for construction a large-count contiguous datatype.
A vector type describes a series of adjacent chunks, and a struct type picks up
any remaining data in case the count is not evenly divisible.}
\end{figure}

%In the general case, MPI\_Type\_create\_struct is required, although BigMPI tries to factorize the count into C integers so we can use MPI\_Type\_vector.

%Ticket 423 would improve user experience because it addresses the common case of large counts of built-ins.

\subsection{Reductions}
\label{sec:reductions}

Large-count support for reductions poses a challenge, particularly in the nonblocking case.
The MPI standard stipulates that built-in reduction operations can be used with built-in types
in the case of reductions, which means that performing a reduction on a vector of $N$
doubles using count=$N$ and type=\texttt{MPI\_DOUBLE} is compatible with \texttt{MPI\_SUM},
whereas the same reduction performed using a contiguous datatype to represent the vector
of doubles requires a user-defined reduction operation.
Thus, BigMPI creates user-defined operations corresponding to all the built-in reductions
acting on contiguous datatypes.  Inside of these reduction operations, the datatype is
decoded and the reduction performed using multiple calls to \texttt{MPI\_Reduce\_local}
and the appropriate built-in reduction operation.

Since many MPI implementations have optimized implementations of reductions for
built-in reduction operations, BigMPI turns a single call for a large count into multiple
calls with count less than $2^{31}$, but this only works for the blocking case.
It is not possible to return a single request object associated with more than one
nonblocking operation.  Generalized requests, the MPI-standard way to implement non-blocking operations in a library,  are not a viable alternative for
reasons that have been documented in other work (\cite{latham:grequest-extensions}).


%How do I support MPI\_IN\_PLACE inside of my user-defined reduction?
%I want to use MPI\_Reduce\_local?

%Blocking reductions can use the cleaver \todo{what the heck is a cleaver?}

\subsection{Vector-argument collectives}

All V-collectives turn into ALLTOALLW because different counts implies different large-count types.

ALLTOALLW displacements given in bytes are C int and therefore it is impossible to offset more than 2GB into the buffer.

NEIGHBOR\_ALLTOALLW to the rescue?!?!

MPI\_Neighbor\_alltoallw displacements are MPI\_Aint not int.  This is good.

Neighborhood collectives require special communicators that must be created for each call (and possibly cached).

Must allocate new argument vectors and, in the case of alltoall, we wastefully splat the same value in all locations.

% V-collectives using P2P and RMA

One can follow the definition in MPI to implement all of the V-collectives using P2P.

RMA (with win\_fence synchronization) also works for the V-collectives.

Allgatherv using nproc calls to Bcast also works.

Large-count definitely outside of recursive doubling regime so little to optimize...

% V-collectives - nonblocking issues

None of the aforementioned solutions works for nonblocking because:

What request do we return in the case of P2P or RMA?

Cannot free argument vectors until complete.

Any solution involving generalized requests is untenable for users.  BigMPI might use it.

\subsection{Neighborhood collectives}

Scalar collectives are easy.

V-collectives: map to ALLTOALLW

Same problem as before with nonblocking regarding the allocated argument vectors.

If not for MPI\_Aint displacements in ALLTOALLW, we would have to drop into P2P and MPICH generalized requests.
