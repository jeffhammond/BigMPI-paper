% !TEX root = ../bigmpi.tex

\subsection{Vector-argument collectives}

Vector-argument collectives (henceforth v-collectives) are the generalization of 
\texttt{MPI\_Scatter}, \texttt{MPI\_Gather}, \texttt{MPI\_Alltoall} etc.
when the count but not the datatype varies across processes.
When datatypes are used to support large counts, all of these operations must be
mapped to \texttt{MPI\_Alltoallw} because each large count will be mapped
to a different user-defined datatype, and \texttt{MPI\_Alltoallw} is the only collective
that supports a vector of datatypes.
Using \texttt{MPI\_Alltoallw} to implement, e.g., a large-count \texttt{MPI\_Scatterv} is
particularly inefficient because the former assumes inputs from every process,
whereas the latter only uses the input from the root.
However, the overhead of scanning a vector of counts where all but one is zero
is inconsequential compared to the cost of transmitting a buffer of $2^{31}$ bytes.
%Additionally, such vectors are unlikely to be particularly long since scattering
%a buffer that would run into large-count issues r

%All V-collectives turn into ALLTOALLW because different counts implies different large-count types.

The v-collectives encounter a second, more subtle issue due to the mapping to
\texttt{MPI\_Alltoallw}.  Because this function takes a vector of datatypes, the
displacements into the input and output vectors are given in bytes,
not element count, and the type of this offset is a C integer.
This creates an overflow situation \textit{even in the case
where the input buffer is less than 2 GiB} because a vector of one billion
alternating integers and floats may require an byte-offset in excess of $2^{31}$.
Thus, \texttt{MPI\_Alltoallw} is not an acceptable solution for the large-count
v-collectives because of the likelihood of overflowing in the displacement vector.
The use of C integer instead of \texttt{MPI\_Aint} for the displacement vector in
the  collective operations added prior to MPI-3 is a unfortunate oversight that
cannot be rectified without breaking backwards compatibility.

%ALLTOALLW displacements given in bytes are C int and therefore it is impossible to offset more than 2GB into the buffer.
%NEIGHBOR\_ALLTOALLW to the rescue?!?!
%MPI\_Neighbor\_alltoallw displacements are MPI\_Aint not int.  This is good.
%Neighborhood collectives require special communicators that must be created for each call (and possibly cached).
%Must allocate new argument vectors and, in the case of alltoall, we wastefully splat the same value in all locations.

Fortunately, the overflow issue with displacements in \texttt{MPI\_Alltoallw} is
resolved using the neighborhood collectives introduced in MPI-3, which
use \texttt{MPI\_Aint} for displacements.
On the other hand, neighborhood collectives require an appropriate
communicator, which must be constructed prior to calling \texttt{MPI\_Neighborhood\_alltoallw}.
BigMPI creates a distributed graph communicator using \texttt{MPI\_Dist\_graph\_adjacent}
on-the-fly for every invocation of the large-count v-collectives, which is assumed to be
insignificant overhead compared to the data movement entailed in such an operation.

Thus, the implementation of large-count v-collectives using \texttt{MPI\_Neighborhood\_alltoallw} 
requires two $O(n_{proc})$ setup steps: the first is to allocate and populate the vectors of 
send and receive counts, displacements and datatypes and 
the second is to create a distributed graph communicator.
Creating the vector of datatypes requires $O(n_{proc})$ calls to \texttt{BigMPI\_Type\_contiguous\_x},
which itself requires six MPI calls, although all of these are expected to be inexpensive.

% V-collectives using P2P and RMA
%One can follow the definition in MPI to implement all of the V-collectives using P2P.
%RMA (with win\_fence synchronization) also works for the V-collectives.
%Allgatherv using nproc calls to Bcast also works.
%Large-count definitely outside of recursive doubling regime so little to optimize...

An alternative approach to implementing large-count v-collectives is to map
these to point-to-point operations, although this only works for blocking operations
due to the inability to aggregate requests, as described above.
Given that large-count v-collectives are well outside of the regime where latency-oriented 
optimizations like recursive-doubling are important, this is unlikely to have a significant impact 
on performance and it eliminates the need for some of the $O(n_{proc})$ setup steps.
The MPI standard describes every collective in terms of its implementation 
in terms of send-recv calls; the BigMPI implementation in terms of point-to-point
follows these recipes closely:
(1) nonblocking receives are pre-posted by the root or all ranks as appropriate;
(2) the root or all ranks then call nonblocking send; 
(3) all ranks then call Waitall.
As the large-count BigMPI send-recv functions are used, there is no need for
$O(n_{proc})$ vectors of datatypes, etc. -- only a vector of \texttt{MPI\_Request}
objects for the nonblocking operations is required.

A third implementation of v-collectives is to use RMA (one-sided) that follows
the same traffic pattern as the point-to-point implementation.
In this case, a MPI window must be created associated with the source (target)
buffers and \texttt{MPI\_Get} (\texttt{MPI\_Put}) operations used for moving data.
The most appropriate synchronization mode for mapping collectives to RMA
is \texttt{MPI\_Win\_fence}, although one could use passive target as well.
In the event that a future version of the MPI standard introduces a nonblocking
equivalent of \texttt{MPI\_Win\_fence} or \texttt{MPI\_Win\_unlock\_all}, these
could be used to implement nonblocking v-collectives in terms of RMA; 
at least within MPI-3, we are limited to the blocking case.
The RMA implementation was prototyped in BigMPI but is not currently implemented
because no performance benefit of one-sided is expected due to the current state
of RMA implementations, which map one-sided operations to two-sided ones internally.
However, it is possible that noticeable performance improvements will be
observed if RMA operations exploit RDMA hardware.

While not named as such, \texttt{MPI\_Reduce\_scatter} is a v-collective.
BigMPI currently does not yet support this function
(\texttt{MPI\_Reduce\_scatter\_block} is supported, however)
but it is straightforward to implement in terms of  \texttt{MPI\_Reduce} and
\texttt{MPI\_Scatterv}.

% V-collectives - nonblocking issues

%None of the aforementioned solutions works for nonblocking because:
%What request do we return in the case of P2P or RMA?
%Cannot free argument vectors until complete.
%Any solution involving generalized requests is untenable for users.  BigMPI might use it.

Unfortunately, there is no way to implement nonblocking v-collectives using 
the aforementioned approaches.  In the case of the neighborhood collective
implementation, we cannot free the vector temporaries holding the counts,
displacements and datatypes until the operation has completed.
If callback functions associated with request completion were present in the
MPI standard (see Ref.~\cite{ticket26} for a proposal of this), then it would
be possible to free the temporary buffers using this callback.
As noted already, since on cannot associate a single request with multiple
nonblocking operations, the point-to-point implementation is inviable
for the nonblocking v-collectives.
Finally, all relevant forms of MPI RMA synchronization have blocking semantics
and thus cannot be used to implement nonblocking collectives.

We identify nonblocking v-collectives as the second example
where MPI-3 lacks the necessary features to support large counts.