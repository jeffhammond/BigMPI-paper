% !TEX root = ../bigmpi.tex

\section{Introduction}

The Message Passing Interface~\cite{mpiforum:94, mpiforum:96, mpiforum:09, mpiforum:12} 
defines a broad set of functionality for writing parallel programs, especially across
distributed computing systems.
Now more than 20 years old, MPI continues to be widely used and has met the challenges of
post-petascale computing, including scaling out to millions of cores~\cite{balaji2011mpi}. 
In order to scale up, in terms of problem size, one needs
to be able to describe large working sets.  The existing (count, datatype) pair
works well, until the 'count' exceeds the range of the native integer type
(in the case of the C interface, \texttt{int}, which is 32 bits on most current platforms).
We call this the ``large-count'' problem.

When drafting MPI-3 the MPI Forum took a minimalist approach large-count 
support~\cite{squyres-blog-large-count,ticket265}.
The forum did introduce a handful of MPI\_Foo\_x routines to make rudimentary
large-count support possible.  After lengthy deliberation, the
Forum asserted that ``just use datatypes'' is a sufficient solution for users.
For example, one can describe 4 billion bytes as 1 billion 4-byte integers.
Or, one could use contiguous MPI dataypes to describe 16 billion bytes as 1,000
16 million-byte chunks.  For these simple examples it is easy to envision a
solution. It is only through implementing the proposed approach for all cases
in MPI that one discovers the challenges hidden in such an assertion.

BigMPI provides a high-level library that supports large counts and can be used as 
drop-in replacement for existing MPI routines.  It was written to test the Forum's assertion
that datatypes are sufficient for large-count support and to provide a drop-in library
solution for applications requiring large-count support.
In this context, ``large-count'' is any count that exceeds \texttt{INT\_MAX}. 

BigMPI is designed for the common case where one has a 64-bit address
space and is unable to do MPI communication on more than $2^{31}$ elements
despite having sufficient memory to allocate such buffers.
%BigMPI does not attempt to support large-counts on systems where
%C \texttt{int} and \texttt{void\*} are both 32 bits.
As systems with more than $2^{63}$ bytes (8192 petibytes) of memory 
per node are unlikely to exist for the foreseeable future --
the total system memory capacity for an exascale machine has been 
predicted to be 50-100 petabytes~\cite{shalf2011exascale} --
BigMPI makes no attempt to support the full range of \texttt{MPI\_Count}
(possibly a 128-bit integer) internally, but rather uses \texttt{MPI\_Aint}
(a 64-bit integer on a system with 64-bit addresses, by definition).

\subsection{Motivation}

The MPI standard provides a wide range of communication functions that
take a C \texttt{int} argument for the element count, thereby limiting this
value to \texttt{INT\_MAX} or less.
This means that one cannot send, e.g. 3 billion bytes using the \texttt{MPI\_BYTE} 
datatype, or a vector of 5 billion integers using the \texttt{MPI\_INT} type, as
two examples.

These limitations may seem academic in nature, as 2 billion
\texttt{MPI\_DOUBLE} equate to 16GB and one might think that
applications may rarely ever need to transmit that much data, as there
may be less RAM available for the whole NUMA domain in which the MPI
process is running. Two recent trends may render this limit
increasingly impractical: first, growing core counts per CPU mean
larger data portions per MPI process and second, Big Data applications
may demand more RAM per core than the traditional 2GB for computer
simulations.

If the user code manually packs data, either for
performance~\cite{jenkins2012enabling} or encoding
reasons~\cite{boostmpi}, then the MPI implementation may be given just
an array of \texttt{MPI\_BYTE}, which further reduces the maximum
message size (e.g. 250 million for \texttt{MPI\_DOUBLE}).

A natural workaround is to use MPI derived datatypes. While it is
plausible that application developers will know typical data sizes and
can thus intercept calls which may exceed the \texttt{INT\_MAX} limit,
another scenario is harder to solve: problem solving
environments~\cite{cactus:SC01, gromacs} and computational
libraries~\cite{physis, libgeodecomp} operate on data structures with
user-defined dimensions. To ensure correctness, developers would need
to safeguard all communication functions which operate on user data.

%As we will demonstrate in the following sections, this is not a
%trivial task for all communication operations. A generic, reusable
%solution as provided by BigMPI alleviates the required implementation
%effort; library users can use preprocessor directives in a header file
%to select the appropriate set of communication functions:
%
%\begin{code}
%// configuration header:
%#ifdef BIGMPI
%#define MPIX_Bcast_x MPIX_Bcast_x
%#define MPIX_Send_x MPIX_Send_x
%...
%#else // cannot use count>INT_MAX
%#define MPIX_Bcast_x MPI_Bcast
%#define MPIX_Send_x MPI_Send
%...
%#endif
%\end{code}

This project aspires to make it as easy as possible to support arbitrarily
large counts ($2^{63}$ elements exceeds the local storage capacity of computers
for the foreseeable future).

% This text sucks

% One area where MPI is less extensive than desirable is operations on very large data.
% Specifically, MPI uses the native integer in the C (\texttt{int}) and Fortran (\texttt{INTEGER})
% interfaces to represent the number of data elements to be communicated;
% in some cases, this type is also used to express an offset (more on this later).  

This paper focuses on the issues with the C interface and we use the
well-known convention I$n_{I}$L$n_{L}$P$n_{P}$ to refer to the sizes
of the C types \texttt{int}, \texttt{long}, and \texttt{void*}, respectively.
For ILP32 systems, the largest buffer one can allocate is $2^{32}$ bytes (4GiB),
while MPI can handle buffers of up to 2GiB; the factor of two difference is
almost never a problem since 4GiB of \texttt{int} for example requires only
a count of only $2^{30}$.

% A problem emerges in IL32P64 and I32LP64 systems because it is possible to allocate
% more memory in a buffer than can be captured with an integer count and built-in datatype.
% For example, a vector of 3 billion floats requires 12 GB of memory but cannot be 
% communicated using any communication routine using built-in datatypes.

