% !TEX root = ../bigmpi.tex

\section{Introduction}

The Message Passing Interface~\cite{mpiforum:94, mpiforum:96, mpiforum:09, mpiforum:12} 
defines a broad set of functionality for writing parallel programs, especially across
distributed computing systems.

Now more than 20 years old, MPI continues to be widely used and has met the challenges of
post-petascale computing, including scaling to millions of cores CITE STUFF.



MPI-3 largely punted on large-count support

A handful of MPI\_Foo\_x routines make rudimentary large-count support possible

Forum asserted that ?just use datatypes? is a sufficient solution for users.

Obviously, no one bothered to verify the aforementioned assertion...

BigMPI provides a high-level library that supports large counts that can be used as 
drop-in replacement for existing MPI routines and test the Forum?s assertion 
that datatypes are sufficient for large-count support.



Interface to MPI for large messages, i.e. those where the count argument
exceeds \texttt{INT\_MAX} but is still less than \texttt{SIZE\_MAX}.
BigMPI is designed for the common case where one has a 64b address
space and is unable to do MPI communication on more than $2^{31}$ elements
despite having sufficient memory to allocate such buffers.
BigMPI does not attempt to support large-counts on systems where
C \texttt{int} and \texttt{void\*} are both 32b.

Motivation

The MPI standard provides a wide range of communication functions that
take a C \texttt{int} argument for the element count, thereby limiting this
value to \texttt{INT\_MAX} or less.
This means that one cannot send, e.g. 3 billion bytes using the \texttt{MPI\_BYTE} 
datatype, or a vector of 5 billion integers using the \texttt{MPI\_INT} type, as
two examples.
There is a natural workaround using MPI derived datatypes, but this is
a burden on users who today may not be using derived datatypes.

This project aspires to make it as easy as possible to support arbitrarily
large counts ($2^{63}$ elements exceeds the local storage capacity of computers
for the foreseeable future).

This is an example of the code change required to support large counts using
BigMPI:
\begin{code}
#ifdef BIGMPI
    MPIX_Bcast_x(buf, large_count, MPI_BYTE, 0, mycomm);
#else // cannot use count>INT_MAX
    MPI_Bcast(buf, small_count, MPI_BYTE, 0, mycomm);
#endif
\end{code}



% This text sucks

One area where MPI is less extensive than desirable is operations on very large data.
Specifically, MPI uses the native integer in the C (\texttt{int}) and Fortran (\texttt{INTEGER})
interfaces to represent the number of data elements to be communicated;
in some cases, this type is also used to express an offset (more on this later).  
This paper focuses on the issues with the C interface and we use the
well-known convention I$n_{I}$L$n_{L}$P$n_{P}$ to refer to the sizes
of the C types \texttt{int}, \texttt{long}, and \texttt{void*}, respectively.
For ILP32 systems, the largest buffer one can allocate is $2^{32}$ bytes (4GiB),
while MPI can handle buffers of up to 2GiB; the factor of two difference is
almost never a problem since 4GiB of \texttt{int} for example requires only
a count of only $2^30$.
A problem emerges in IL32P64 and I32LP64 systems because it is possible to allocate
more memory in a buffer than can be captured with an integer count and built-in datatype.
For example, a vector of 3 billion floats requires 12 GB of memory but cannot be 
communicated using any communication routine using built-in datatypes.

